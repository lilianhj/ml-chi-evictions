{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evictions data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read in the original dataset from the Evictions Lab, limit it to only block groups within Cook County (we will later limit this to Chicago), and construct a census tract ID to facilitate future joining to the ACS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_groups = pd.read_csv('raw_data/block-groups.csv',\n",
    "                           dtype={'GEOID': str})\n",
    "\n",
    "#Restricting evictions data to Cook County\n",
    "evictions = block_groups[block_groups['parent-location'] == 'Cook County, Illinois'].copy()\n",
    "\n",
    "#Creating a census tract ID by removing the block group signifier \n",
    "evictions['tract_id'] = evictions['GEOID'].apply(lambda x: x[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Cook County Crime Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the City of Chicago data portal API to download crime data for Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sodapy import Socrata\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reading in selected columns from the Chicago Data Portal\n",
    "between 2001-present\n",
    "'''\n",
    "crime = pd.read_csv('raw_data/Crimes_-_2001_to_present.csv',\n",
    "                    usecols=['Date', 'Primary Type', 'Description', 'Community Area', 'Zip Codes', 'Location'],\n",
    "                    dtype={'Community Area': str},\n",
    "                    parse_dates=['Date'], date_parser = pd.to_datetime)\n",
    "\n",
    "#Subsetting data to match our evictions timeframe\n",
    "crime = crime[crime.Date < '01/01/2017']\n",
    "\n",
    "#removing 57,388 row missing Location data that we would be unable to\n",
    "#merge with ACS and therefore eviction data\n",
    "crime = crime[~crime.Location.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In order to better interpret our data, we rename 'community area' codes\n",
    "to their respective neighborhood names.\n",
    "'''\n",
    "\n",
    "#Interacting with API to get community area to neighborhood conversions\n",
    "client = Socrata(\"data.cityofchicago.org\", None)\n",
    "neighborhoods = client.get(\"igwz-8jzy\", select=\"AREA_NUMBE, COMMUNITY\")\n",
    "\n",
    "#Creating dictionary mapping community area to neighborhood\n",
    "neighborhood_dict = pd.DataFrame.from_records(neighborhoods, index=\"AREA_NUMBE\").to_dict()[\"COMMUNITY\"]\n",
    "\n",
    "#Note that we name area '0' as 'OHARE' as O'Hare was not listed in the data portal\n",
    "neighborhood_dict['0'] = \"OHARE\"\n",
    "\n",
    "#Renaming to neighborhoods\n",
    "crime['Community Area'] = crime['Community Area'].replace(neighborhood_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning ACS Demographic Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain certain demographic variables (American Community Survey estimates) through the census API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from census import Census\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_key = \"1006a1e4567f2582e57c3affbab639c19c930271\"\n",
    "acs_bgs = []\n",
    "acs_tracts = []\n",
    "\n",
    "for time in range(2010,2017):\n",
    "    acs = Census(acs_key, year=time)\n",
    "\n",
    "    '''\n",
    "    Due to changes in ACS data over the years, the following variables were\n",
    "    unavailable for download before 2013\n",
    "    '''\n",
    "    if time >= 2013:\n",
    "        req_bg = acs.acs5.get(('NAME', 'B25010_002E', 'B25010_003E'), \n",
    "                              {'for': 'block group:*', 'in': 'state:17 county:031'})\n",
    "        \n",
    "        block_groups = pd.DataFrame(req_bg)\n",
    "        block_groups = block_groups.rename(columns={\n",
    "                            \"B25010_002E\": \"avg_household_size_owner\",\n",
    "                            \"B25010_003E\": \"avg_household_size_renter\"})\n",
    "        \n",
    "        #Creating a column for year an ACS variable pulled from\n",
    "        block_groups['year'] = time\n",
    "        acs_bgs.append(block_groups)\n",
    "    \n",
    "    #Note that these variables are census tract level\n",
    "    req_tracts = acs.acs5.get(('NAME','B06009_002E', 'B06009_003E', 'B06009_004E',\n",
    "                           'B06009_005E', 'B23020_001E'), \n",
    "                       {'for': 'tract:*', 'in': 'state:17 county:031'})\n",
    "\n",
    "    tracts = pd.DataFrame(req_tracts)\n",
    "    tracts = tracts.rename(columns={\n",
    "                            \"B06009_002E\": \"Total_Less_Than_HS\",\n",
    "                            \"B06009_003E\": \"Total_HS_Grad\",\n",
    "                            'B06009_004E': 'Total_Some_College_or_AAS',\n",
    "                            'B06009_005E': 'Total_Bachelors',\n",
    "                            'B23020_001E': 'Mean_hours_worked'})\n",
    "    tracts['year'] = time\n",
    "    acs_tracts.append(tracts)\n",
    "    \n",
    "tracts_df = pd.concat(acs_tracts)\n",
    "bg_df = pd.concat(acs_bgs)\n",
    "\n",
    "\n",
    "#Create a unique id to identify each block group or tract\n",
    "bg_df['bgid'] = (bg_df.state + bg_df.county\n",
    "                        + bg_df.tract + bg_df['block group'])\n",
    "\n",
    "tracts_df['tract_id'] = (tracts_df.state + tracts_df.county\n",
    "                        + tracts_df.tract)\n",
    "\n",
    "'''\n",
    "Set 234 negative values from 'avg_household_size_owner' and\n",
    "1,125 negative values from 'avg_household_size_renter' to NaN\n",
    "'''\n",
    "for col in bg_df.columns[:2]:\n",
    "    bg_df.loc[bg_df[col] < 0, col] = np.nan\n",
    "\n",
    "#Set 32 negative values from 'Mean_hours_worked' to NaN\n",
    "for col in tracts_df.columns[:5]:\n",
    "    tracts_df.loc[tracts_df[col] < 0, col] = np.nan\n",
    "    \n",
    "#Fill NaNs with preceding value\n",
    "bg_df.fillna(method='ffill', inplace=True)\n",
    "tracts_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "#Remove extraneous columns from bg_df and tracts_df\n",
    "bg_df = bg_df.drop(['NAME', 'block group', 'county', 'state', 'tract'], axis=1)\n",
    "tracts_df = tracts_df.drop(['NAME', 'county', 'state', 'tract'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two separate dataframes containing ACS data between 2009-2016 at the Census Tract level and ACS data between 2013-2016 at the block group level. We wish to impute values so our ACS data will match the size of our Evictions data. After investigating several of our ACS variables, we decided that calculating time trends through regression analysis would not be useful as the small number of data points caused unrepresentative patterns. However, it is not sufficient to simply impute the median or mean value for time series data. Thus, we calculate the percent change in population between block groups over time from our Eviction dataframe as a proxy for assumed change in ACS variables.\n",
    "\n",
    "We impute at this point (before carrying out train/test splits) because our imputation is carried out based on the time trend within that particular block group, i.e. extrapolating backwards from future data - the very nature of our imputation method makes it impossible to carry out without the full set of available data from the future. In addition, since we are only looking at future values within that specific block group, it does not matter that we have not yet filtered our dataset to only contain block groups within the city of Chicago (rather than Cook County as a whole) - there is no leakage of information from other block groups, again because our imputation method is solely concerned with the time trend within that single block group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_impute_dict(data, group_id):\n",
    "    '''\n",
    "    Create a dictionary of dictionaries\n",
    "    for a given dataframe\n",
    "    '''\n",
    "    pct_changes = data.groupby([group_id, 'year']).population.sum().pct_change().reset_index()\n",
    "    \n",
    "    '''\n",
    "    Setting pct_change to NaN in the year 2000, so there is no\n",
    "    change found between GEOID groups\n",
    "    '''\n",
    "    pct_changes.loc[pct_changes['year'] == 2000, 'population'] = np.nan\n",
    "    \n",
    "    return pct_changes.groupby(group_id).apply(lambda x: dict(zip(x['year'], x['population']))).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(data, cols, year, impute_dict):\n",
    "    '''\n",
    "    Impute previous years of data using population change\n",
    "    as a proxy for other variables.\n",
    "    \n",
    "    Iteratively creates a new dataframe with a given list of columns\n",
    "    \n",
    "    Inputs:\n",
    "        data (dataframe)\n",
    "        cols (list of columns)\n",
    "        year (int)\n",
    "        impute_dict (dictionary)\n",
    "    \n",
    "    Returns:\n",
    "        a dataframe\n",
    "    '''\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    \n",
    "    i = 0\n",
    "    for name, group in data.groupby(cols[-1]):\n",
    "        if name in impute_dict:\n",
    "            curr = group[group['year'] == year][cols[0]].squeeze()\n",
    "            prev = curr\n",
    "            for time in range(year, 2000, -1):\n",
    "                scalar = (1 - impute_dict[name][time])\n",
    "                df.loc[i] = [prev * scalar, time - 1, name]\n",
    "                prev = prev * scalar\n",
    "                i += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating imputation dictionaries for block and tract ids \n",
    "bg_impute_dict = create_impute_dict(evictions, 'GEOID')\n",
    "tract_impute_dict = create_impute_dict(evictions, 'tract_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_impute_list = []\n",
    "for column in bg_df.columns[:2]:\n",
    "    bg_impute_list.append(impute_data(bg_df, [column, 'year', 'bgid'], 2013, bg_impute_dict))\n",
    "    \n",
    "bg_imputes = pd.merge(bg_impute_list[0], bg_impute_list[1], on=['bgid','year'])\n",
    "bg_imputes = bg_imputes[['avg_household_size_owner', 'avg_household_size_renter', 'year', 'bgid']]\n",
    "\n",
    "bg_df = pd.concat([bg_df, bg_imputes]).sort_values(by=['bgid', 'year'])\n",
    "\n",
    "#Adding a tract_id column for merging later\n",
    "bg_df['tract_id'] = bg_df['bgid'].apply(lambda x: x[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_impute_list = []\n",
    "for column in tracts_df.columns[:5]:\n",
    "    tracts_impute_list.append(impute_data(tracts_df, [column, 'year', 'tract_id'], 2010, tract_impute_dict))\n",
    "\n",
    "tract_imputes = reduce(lambda x, y: pd.merge(x,y, on=['tract_id', 'year']), tracts_impute_list)\n",
    "tract_imputes = tract_imputes[['Total_Less_Than_HS', 'Total_HS_Grad', 'Total_Some_College_or_AAS',\n",
    "                               'Total_Bachelors', 'Mean_hours_worked', 'year', 'tract_id']]\n",
    "\n",
    "tracts_df = pd.concat([tracts_df, tract_imputes]).sort_values(by=['tract_id', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we merge our ACS data into one dataframe\n",
    "demographics = pd.merge(bg_df, tracts_df, on=['tract_id', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatially merging Crime data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aggregate data by block group to get the most common crimes and total number of crimes in each block group. We accomplish this through a spatial join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_location(location):\n",
    "    '''\n",
    "    Takes the 'Location' column from the Crime dataset\n",
    "    and turns it into Point objects with corresponding\n",
    "    Longitude and Latitude \n",
    "    '''\n",
    "    lat, long = location.split(',')\n",
    "\n",
    "    return Point(float(long[:-1]), float(lat[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_block_groups(dataframe, n=5, block_group=None):\n",
    "    '''\n",
    "    Identifies the top n crimes reported in a chosen set of block groups for\n",
    "    comparison across time. Default call will return top 5 crimes for all block groups\n",
    "    in all years.\n",
    "    \n",
    "    Inputs:\n",
    "        dataframe\n",
    "        n (int): desired top n crimes\n",
    "        block_group (list): list of block_groups to report\n",
    "\n",
    "    Returns:\n",
    "        series - top n crimes for specified block groups\n",
    "    '''\n",
    "    areas = dataframe.groupby([\"GEOID\", dataframe.Date.dt.year, \"Primary Type\"])['Primary Type'].count()\n",
    "    groups = areas.groupby([\"GEOID\",\"Date\"]).nlargest(n).reset_index(level=[1,2], drop=True)\n",
    "    \n",
    "    if block_group:\n",
    "        return groups[neighborhood]\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the shapefile for block groups in Chicago\n",
    "block_group_shapes = gpd.read_file(\"raw_data/cb_2017_17_bg_500k/cb_2017_17_bg_500k.shp\")\n",
    "\n",
    "#Creating a new geometry column of Point objects\n",
    "crime['geometry'] = crime.Location.apply(lambda x: split_location(x))\n",
    "\n",
    "#Creating a geospatial dataframe\n",
    "crime_geo = gpd.GeoDataFrame(crime, geometry='geometry')\n",
    "\n",
    "#Adjusting the coordinate reference system to match the shapefile\n",
    "crime_geo.crs = {'init': 'epsg:4269'}\n",
    "\n",
    "#Spatially joining\n",
    "spcrime = gpd.sjoin(crime_geo, block_group_shapes, how='left', op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine top crime in a block group\n",
    "top_crime = pd.DataFrame(check_block_groups(spcrime, 1))\n",
    "top_crime.rename(columns={'Primary Type': 'Count'}, level=0, inplace=True)\n",
    "top_crime.reset_index(inplace=True)\n",
    "top_crime = top_crime[['GEOID', 'Date', 'Primary Type']]\n",
    "\n",
    "#Determine the total amount of crimes in a block group\n",
    "crime_sums = pd.DataFrame(spcrime.groupby([\"GEOID\", spcrime.Date.dt.year, \"Primary Type\"])['Primary Type'].count())\n",
    "crime_sums.rename(columns={'Primary Type': 'Sum'}, inplace=True)\n",
    "crime_sums = crime_sums.groupby(['GEOID', 'Date']).sum().reset_index()\n",
    "\n",
    "#Merge top_crime and crime_sums into one dataframe\n",
    "crime_aggregates = pd.merge(top_crime, crime_sums, on=['GEOID', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Crime Data for 2000:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we lack crime data for 2000, we impute it using regression imputation. Again, since we are looking at the time trend within block groups, the imputation can occur at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent(data, name, col):\n",
    "    '''\n",
    "    Returns the most recent information for a given\n",
    "    column, in the even that particular years are missing\n",
    "    \n",
    "    Inputs:\n",
    "        data (dataframe)\n",
    "        name (str): a GEOID\n",
    "        col (str): desired column\n",
    "    \n",
    "    Returns:\n",
    "        str or int, depending on column\n",
    "        np.nan if no values match\n",
    "    '''\n",
    "    for year in range(2001, 2017):\n",
    "        recent = data[(data['GEOID'] == name) &\n",
    "                      (data.Date == year)][col].squeeze()\n",
    "\n",
    "        if not isinstance(recent, pd.Series):\n",
    "            return recent\n",
    "\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_crime(dataframe, cols, time_trends, year):\n",
    "    '''\n",
    "    Creates a datframe with the predicted value for Sums and\n",
    "    most recent value for primary crime type. Iterates over\n",
    "    different time series trends and chooses the regression\n",
    "    model with the highest adjusted r-squared term for prediction.\n",
    "    \n",
    "    Inputs:\n",
    "        dataframe\n",
    "        cols (list)\n",
    "        time_trends (list)\n",
    "        year (int)\n",
    "    \n",
    "    Returns:\n",
    "        dataframe\n",
    "    '''\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    i = 0\n",
    "    for name, group in dataframe.groupby('GEOID'):\n",
    "        evals = []\n",
    "        models = []\n",
    "        for time in time_trends:\n",
    "            model = smf.ols(formula='Sum ~ ' + time,\n",
    "                            data = dataframe[dataframe['GEOID'] == name]).fit()\n",
    "            evals.append(model.rsquared_adj)\n",
    "            models.append(model)\n",
    "        impute = models[evals.index(max(evals))].predict(exog=dict(Date=year))\n",
    "        impute = impute.squeeze()\n",
    "        \n",
    "        #Imputes most recent 'Sum' for the 4 cases where negative values\n",
    "        #were predicted due to overfitting\n",
    "        if impute <= 0:\n",
    "            impute = get_most_recent(crime_aggregates, name, 'Sum')\n",
    "        p_type = get_most_recent(crime_aggregates, name, 'Primary Type')\n",
    "        df.loc[i] = [name, year, p_type, impute]\n",
    "        i += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1549: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1550: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  * (1 - self.rsquared))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1543: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 1 - self.ssr/self.centered_tss\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1543: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 1 - self.ssr/self.centered_tss\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1549: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We found iterating over linear and quadratic\n",
    "time trends was more effective than including higher\n",
    "order polynomial terms, which tended to overfit the \n",
    "data and often predicted negative crime values\n",
    "'''\n",
    "time_trends = ['Date', 'Date + np.power(Date, 2)']\n",
    "\n",
    "#Imputing data for 2001\n",
    "crime_imputes = impute_crime(crime_aggregates, ['GEOID', 'Date', 'Primary Type', 'Sum'], time_trends, 2000 )\n",
    "\n",
    "full_crime = pd.concat([crime_aggregates, crime_imputes]).sort_values(by=['GEOID', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging crime, demographics, and eviction data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We merge our evictions, ACS (demographic), and crime data into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming demographics columns for merge\n",
    "demographics.rename(columns={'bgid':'GEOID', 'year': 'Date'}, inplace=True)\n",
    "evictions.rename(columns={'year': 'Date'}, inplace=True)\n",
    "\n",
    "#Merging demographics and crime dataframes\n",
    "acs_crime = pd.merge(demographics, full_crime, on=['GEOID', 'Date'], how='left')\n",
    "\n",
    "#Casting types to match evictions dataframe\n",
    "acs_crime['Date'] = acs_crime['Date'].astype(int)\n",
    "\n",
    "#Merging evictions and ACS/Crime dataframes\n",
    "evictions = pd.merge(evictions, acs_crime, on=['GEOID', 'Date'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and merging redlining data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyles\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#Ensuring block group shapefile is ready for spatial join\n",
    "block_group_shapes['geometry'] = block_group_shapes['geometry'].apply(Polygon)\n",
    "\n",
    "#Reading in redlining data\n",
    "holc = gpd.read_file('raw_data/HOLC_Chicago/HOLC_Chicago.shp')\n",
    "\n",
    "holc.crs = {'init': 'epsg:4269'}\n",
    "\n",
    "#By joining the block groups shapefile to the redlining shapefile,\n",
    "#we find which block groups are located in redlined areas.\n",
    "intersections = gpd.sjoin(block_group_shapes, holc, how=\"left\", op=\"intersects\")\n",
    "\n",
    "'''\n",
    "Since 50% of Chicago was classified as C (\"Definitely Declining\") by\n",
    "the Home Owners' Loan Corporation, we define a \"redlined\" area as\n",
    "one classified as D (\"Hazardous\") by the HOLC.\n",
    "'''\n",
    "intersections['redlined'] = np.where(intersections['holc_grade'] == 'D', 1, 0)\n",
    "\n",
    "#Finding redlined block groups\n",
    "redlined = intersections[intersections['redlined'] == 1].copy()\n",
    "\n",
    "#Restructuring dataframe for merge\n",
    "redlined_fin = redlined.drop_duplicates(['GEOID'])\n",
    "redlined_fin = redlined_fin[[\"GEOID\", \"redlined\", \"holc_grade\", \"holc_id\"]]\n",
    "\n",
    "#Merging combined evictions dataframe and redlining data\n",
    "evictions_with_red = pd.merge(evictions, redlined_fin, on='GEOID', how='left')\n",
    "\n",
    "#Labeling non-redlined block groups with 0\n",
    "evictions_with_red['redlined'] = evictions_with_red['redlined'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricting scope of data to within the city of Chicago:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We download a dataset listing all census blocks within the city of Chicago, from \n",
    "https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Census-Blocks-2010/mfzt-js4n\n",
    "'''\n",
    "chicago_blocks = pd.read_csv(\"raw_data/chicago_blocks.csv\", dtype=str)\n",
    "\n",
    "'''\n",
    "Since the block group ID is not in the City of Chicago dataset, we construct it\n",
    "from: state ID, county ID, census tract ID, and the first digit of the census block ID.\n",
    "'''\n",
    "chicago_blocks['GEOID'] = (chicago_blocks[\"STATEFP10\"] + chicago_blocks[\"COUNTYFP10\"] +\n",
    "                           chicago_blocks[\"TRACTCE10\"] + chicago_blocks[\"BLOCKCE10\"].str[0])\n",
    "\n",
    "chicago_blocks.drop_duplicates(['GEOID'], inplace=True)\n",
    "\n",
    "#Final merge to restrict the domain of our data to the city of Chicago\n",
    "full_data_chicago = pd.merge(evictions_with_red, chicago_blocks[['GEOID']],\n",
    "                             on='GEOID', how='inner')\n",
    "\n",
    "#Writing results to csv\n",
    "full_data_chicago.to_csv('full_data_chicago.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
